---
title: "Running AI on My Mac: Why I Ditched ChatGPT for LM Studio"
date: "2025-10-31"
excerpt: "How I run powerful AI models locally on my M4 Pro MacBook with LM Studio. Privacy, speed, and vision models that can see images - all offline."
tags: ["AI", "LM Studio", "Privacy", "Mac", "Qwen", "Local AI"]
published: true
---

I was knee-deep in a coding problem when ChatGPT went dark. The little error message mocked me. "Try again later," it said. My deadline wasn't going to wait.

That outage was annoying. But it got me thinking. Every time I use ChatGPT, my data flies to OpenAI's servers. They log it. They train on it. Maybe that's fine for "what's a good pizza recipe." But code snippets? Research notes? That felt wrong.

I needed something different. Something local. Something mine.

## The Cloud Problem No One Talks About

Here's the thing about cloud AI. It's convenient. Type a question, get an answer. But convenience has a cost.

Your prompts live on someone else's computer. They say they don't use it for training anymore. Maybe they don't. But can you be sure? What about government requests? Data breaches? Server logs?

I'm not paranoid. I just value control. My Mac has plenty of power. Why send my data across the internet when I can process it right here?

## How I Found LM Studio

A friend mentioned LM Studio in passing. "Run AI models on your Mac," he said. "Totally free."

I was skeptical. Local AI sounded slow. Complicated. Probably worse than the cloud versions.

I downloaded it anyway. The install was simple. No account. No credit card. Just download and go.

The interface looked clean. Like a chat app, but with a model picker. I could browse thousands of open-source models. Download them. Run them locally.

No API keys. No monthly bills. No data leaving my machine.

## My M4 Pro Makes This Possible

I upgraded to an M4 Pro MacBook Pro earlier this year. 48GB of unified memory. At the time, I thought I'd gone overboard.

Turns out, that RAM is perfect for AI. Most people run smaller models. Maybe 8B parameters. Those work fine on 16GB machines.

But with 48GB? I can run Qwen3-VL-30B. That's a big model. A smart model. And it can do something most AI tools can't.

It can see.

## Vision Models Changed Everything

Qwen3-VL is a vision-language model. Feed it text, it responds. Feed it an image, it understands that too.

Last week I got a weird error in my terminal. Red text everywhere. I took a screenshot. Pasted it into LM Studio. Asked "what's wrong here?"

The model looked at my screenshot. Pointed out the exact line causing trouble. Explained why. Suggested a fix.

This isn't OCR. The model actually understands visual context. Diagrams. Charts. UI mockups. Code screenshots with syntax highlighting intact.

I also run the smaller Qwen3-VL-8B. It's faster. Good for quick questions. But the 30B model? That's where the magic happens. Complex reasoning. Better context. More accurate answers.

Most people can't run 30B models. Not enough RAM. I can. That's my edge.

## What I Actually Use It For

Every day looks different. Sometimes I'm researching a new library. I paste documentation. Ask questions. The model helps me understand faster than reading alone.

Other times I'm experimenting. Testing prompts. Trying different models. Seeing what works. LM Studio makes this easy. Switch models with one click.

I've used it for code reviews. Explaining legacy code. Brainstorming architecture. Even writing. The AI isn't perfect. But it's always there. Always private.

Yesterday I analyzed a competitor's UI design. Screenshot → LM Studio → detailed breakdown of their layout choices. All offline. No one tracking what I'm researching.

That's the real win. Privacy isn't about hiding. It's about control.

## The Honest Downsides

LM Studio isn't perfect. Let me be real about the problems.

First, it's slower than ChatGPT. Cloud models have massive GPU clusters. My Mac has... one M4 Pro. Responses take longer. Maybe 5-10 seconds instead of instant.

Second, you manage everything yourself. Want a new model? Download it. That's a few gigabytes. Models pile up fast. You'll need storage space.

Third, there's a learning curve. Which model for which task? What's the difference between 7B and 70B? You have to learn this stuff.

The interface is simpler than the web version of ChatGPT. No plugins. No browsing. Just you and the model.

For some tasks, cloud AI still wins. Web search? Up-to-date info? Yeah, ChatGPT is better there.

## Why I Keep Using It Anyway

Because my data stays mine.

Because I don't need internet to think.

Because I'm not paying per token.

I ran the numbers. ChatGPT Plus is $20/month. That's $240/year. LM Studio is free. The models are free. I paid for the Mac anyway.

When my internet goes down, LM Studio still works. When OpenAI has an outage, I don't care. When they change their pricing, doesn't affect me.

I'm not locked into their ecosystem. Don't like Qwen? Try Llama. Or Mistral. Or Deepseek. Or whatever comes next week.

Open source moves fast. Really fast. Models improve constantly. And they're all free.

## Getting Started Is Easier Than You Think

Go to lmstudio.ai. Download the app. Open it.

Click "Discover" to browse models. Search for "Qwen3-VL-8B" if you want to try vision. Or "Qwen-7B" for text-only.

Download a model. Wait. It's big. Go make coffee.

Once downloaded, click "Load." Pick your model. Start chatting.

That's it. Seriously.

If you have a Mac with Apple Silicon, you're golden. 16GB RAM minimum. More is better. Windows and Linux work too.

The models live at Hugging Face. Thousands of them. Some good. Some bad. LM Studio shows you ratings and downloads to help pick.

## My Final Take

I still use ChatGPT sometimes. It has its place. But for daily work? LM Studio wins.

The M4 Pro with 48GB was expensive. But running serious AI models locally? That alone justifies the cost. The vision models are a game-changer. Most people don't realize what's possible.

If you care about privacy, try it. If you hate subscriptions, try it. If you just want to own your tools, try it.

Your data belongs to you. Not to some company's training dataset.

Download LM Studio. Pick a model. See what local AI can do.

You might not go back.
